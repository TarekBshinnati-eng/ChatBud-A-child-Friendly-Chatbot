{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "755e8630",
   "metadata": {},
   "source": [
    "# HarmEval Benchmark Prompt + Label Sampler and EDA\n",
    "\n",
    "This notebook:\n",
    "- Loads `SoftMINER-Group/HarmEval`.\n",
    "- Samples 500 rows with a fixed random seed (17).\n",
    "- Saves a CSV with columns `Question` and `Topic`.\n",
    "- Includes a small EDA section: question length stats, topic distribution, and simple plots.\n",
    "\n",
    "It does **not** run any models; it only prepares and analyzes the benchmark data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73fd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q datasets pandas matplotlib seaborn\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "SEED = 17\n",
    "np.random.seed(SEED)\n",
    "\n",
    "OUTPUT_DIR = \".\"  # current folder: 'toxicity metric'\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6ab4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HarmEval dataset\n",
    "harmeval = load_dataset(\"SoftMINER-Group/HarmEval\")\n",
    "print(harmeval)\n",
    "\n",
    "if \"test\" in harmeval:\n",
    "    harmeval_split = harmeval[\"test\"]\n",
    "elif \"validation\" in harmeval:\n",
    "    harmeval_split = harmeval[\"validation\"]\n",
    "else:\n",
    "    harmeval_split = harmeval[\"train\"]\n",
    "\n",
    "print(\"Using split:\", harmeval_split)\n",
    "print(\"Columns:\", harmeval_split.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c2aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas and sample rows\n",
    "\n",
    "df = harmeval_split.to_pandas()\n",
    "\n",
    "assert \"Question\" in df.columns, \"Expected a 'Question' column in HarmEval.\"\n",
    "assert \"Topic\" in df.columns, \"Expected a 'Topic' column in HarmEval.\"\n",
    "\n",
    "df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "N = min(500, len(df))\n",
    "sample_df = df.iloc[:N].copy()\n",
    "print(\"Number of rows selected:\", len(sample_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e5c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final dataframe with Question and Topic\n",
    "\n",
    "prompts_df = sample_df[[\"Question\", \"Topic\"]].copy()\n",
    "prompts_df[\"Question\"] = prompts_df[\"Question\"].astype(str)\n",
    "prompts_df[\"Topic\"] = prompts_df[\"Topic\"].astype(str)\n",
    "\n",
    "assert list(prompts_df.columns) == [\"Question\", \"Topic\"]\n",
    "print(prompts_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sampled prompts to CSV\n",
    "\n",
    "output_path = os.path.join(OUTPUT_DIR, \"harmeval_prompts_labeled.csv\")\n",
    "prompts_df.to_csv(output_path, index=False)\n",
    "print(\"Saved labeled HarmEval prompts to:\", output_path)\n",
    "print(\"Total rows in CSV:\", len(prompts_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4820b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: basic statistics\n",
    "\n",
    "prompts_df[\"char_len\"] = prompts_df[\"Question\"].str.len()\n",
    "prompts_df[\"word_len\"] = prompts_df[\"Question\"].str.split().str.len()\n",
    "\n",
    "print(\"=== Question length (characters) ===\")\n",
    "print(prompts_df[\"char_len\"].describe())\n",
    "\n",
    "print(\"\n",
    "=== Question length (words) ===\")\n",
    "print(prompts_df[\"word_len\"].describe())\n",
    "\n",
    "print(\"\n",
    "=== Topic distribution ===\")\n",
    "topic_counts = prompts_df[\"Topic\"].value_counts()\n",
    "print(topic_counts)\n",
    "\n",
    "topic_percent = 100 * topic_counts / len(prompts_df)\n",
    "topic_summary = pd.DataFrame({\n",
    "    \"count\": topic_counts,\n",
    "    \"percent\": topic_percent.round(2),\n",
    "})\n",
    "print(\"\n",
    "=== Topic distribution (count and %) ===\")\n",
    "print(topic_summary)\n",
    "\n",
    "topic_summary_path = os.path.join(OUTPUT_DIR, \"harmeval_topic_summary.csv\")\n",
    "topic_summary.to_csv(topic_summary_path, index_label=\"Topic\")\n",
    "print(\"Saved topic summary to:\", topic_summary_path)\n",
    "\n",
    "length_by_topic = prompts_df.groupby(\"Topic\")[ [\"word_len\", \"char_len\"] ].agg([\"mean\", \"median\"])\n",
    "print(\"\n",
    "=== Question length by Topic (mean/median) ===\")\n",
    "print(length_by_topic)\n",
    "\n",
    "length_by_topic_path = os.path.join(OUTPUT_DIR, \"harmeval_length_by_topic.csv\")\n",
    "length_by_topic.to_csv(length_by_topic_path)\n",
    "print(\"Saved length-by-topic stats to:\", length_by_topic_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174fffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Histogram of word lengths\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(prompts_df[\"word_len\"], bins=30)\n",
    "plt.xlabel(\"Words per question\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of question length (words)\")\n",
    "hist_path = os.path.join(OUTPUT_DIR, \"harmeval_word_len_hist.png\")\n",
    "plt.savefig(hist_path, bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved:\", hist_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc06126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Topic counts bar chart\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(x=topic_counts.index, y=topic_counts.values)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"HarmEval topic distribution\")\n",
    "topic_bar_path = os.path.join(OUTPUT_DIR, \"harmeval_topic_counts.png\")\n",
    "plt.savefig(topic_bar_path, bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved:\", topic_bar_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92870963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Boxplot of word length by topic\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(data=prompts_df, x=\"Topic\", y=\"word_len\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Words per question\")\n",
    "plt.title(\"Question length by topic\")\n",
    "box_path = os.path.join(OUTPUT_DIR, \"harmeval_word_len_by_topic.png\")\n",
    "plt.savefig(box_path, bbox_inches=\"tight\", dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved:\", box_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11792785",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- 500 sampled rows from `SoftMINER-Group/HarmEval` with `Question` and `Topic`.\n",
    "- CSV file: `harmeval_prompts_labeled.csv`.\n",
    "- EDA outputs:\n",
    "  - `harmeval_topic_summary.csv`\n",
    "  - `harmeval_length_by_topic.csv`\n",
    "  - plots: `harmeval_word_len_hist.png`, `harmeval_topic_counts.png`, `harmeval_word_len_by_topic.png`.\n",
    "\n",
    "These statistics and plots can be used directly in the reportâ€™s EDA section for the toxicity benchmark. This notebook does **not** perform any model generation or DeepEval code; it only prepares and analyzes the benchmark data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
