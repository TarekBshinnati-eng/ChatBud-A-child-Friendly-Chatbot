{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "# ChatBud - Child-Friendly AI Assistant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell1_header"
   },
   "source": [
    "## Cell 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers>=4.50.0 accelerate>=1.0.0 peft>=0.12.0 bitsandbytes>=0.43.3\n",
    "!pip install -q huggingface_hub sentencepiece\n",
    "!pip install -q flask flask-cors Pillow\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    raise RuntimeError(\"\u274c No GPU! Go to Runtime \u2192 Change runtime type \u2192 T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell2_header"
   },
   "source": [
    "## Cell 2: Login to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf_login"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "except Exception:\n",
    "    hf_token = None\n",
    "\n",
    "if not hf_token:\n",
    "    hf_token = \"YOUR_HF_TOKEN\"\n",
    "\n",
    "if hf_token and hf_token != \"YOUR_HF_TOKEN\":\n",
    "    login(token=hf_token)\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "    print(\"\u2705 Logged in to Hugging Face\")\n",
    "else:\n",
    "    raise ValueError(\"Set HF_TOKEN in Colab secrets or replace YOUR_HF_TOKEN with your token.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell3_header"
   },
   "source": [
    "## Cell 3: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "ADAPTER_DIR = \"/content/drive/MyDrive/gemma3_child_friendly_lora/gemma3_child_friendly_lora\"\n",
    "\n",
    "if os.path.exists(ADAPTER_DIR):\n",
    "    print(f\"\u2705 Adapter found: {ADAPTER_DIR}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"\u274c Adapter not found: {ADAPTER_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell4_header"
   },
   "source": [
    "## Cell 4: Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_base_model"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "MODEL_ID = \"google/gemma-3-4b-it\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "print(\"\u2705 Processor loaded\")\n",
    "\n",
    "print(\"\\nLoading model (2-3 min)...\")\n",
    "base_model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "base_model.eval()\n",
    "print(\"\u2705 Base model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell5_header"
   },
   "source": [
    "## Cell 5: Load LoRA + Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_lora"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "print(f\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
    "model.eval()\n",
    "print(\"\u2705 Fine-tuned model ready!\")\n",
    "\n",
    "# =============================================================================\n",
    "# SYSTEM PROMPT (hardcoded for child safety)\n",
    "# =============================================================================\n",
    "SYSTEM_PROMPT = \"\"\"You are ChatBud, a friendly and safe helper for children aged 9\u201311.\n",
    "Speak with simple words (use the least number of words as possible) and short sentences (concise), like you're talking to a smart kid, and keep answers brief (about 1\u20134 short sentences as a maximum).\n",
    "Never swear, use rude or sexual language, or describe violence, self-harm, or sex in graphic detail.\n",
    "Do not give risky instructions, dares, or tips that could hurt someone in real life or online.\n",
    "If a problem sounds serious or scary, tell the child to stop, stay safe, and talk to a trusted adult such as a parent, caregiver, teacher, or counselor.\"\"\"\n",
    "\n",
    "print(f\"\\n\ud83d\udccb System prompt loaded ({len(SYSTEM_PROMPT)} chars)\")\n",
    "\n",
    "# =============================================================================\n",
    "# Test with system prompt\n",
    "# =============================================================================\n",
    "print(\"\\n\ud83e\uddea Testing model with system prompt...\")\n",
    "try:\n",
    "    test_messages = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What are elephants?\"}]}\n",
    "    ]\n",
    "    \n",
    "    test_inputs = processor.apply_chat_template(\n",
    "        test_messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    print(f\"   Input tokens: {test_inputs['input_ids'].shape[-1]}\")\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        test_output = model.generate(\n",
    "            **test_inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    \n",
    "    response = processor.decode(test_output[0][test_inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    print(f\"   \u2705 Response: {response[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"   \u274c Test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\u2705 Ready! Run Cell 6 to start server.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell6_header"
   },
   "source": [
    "## Cell 6: Start Server (with Memory + Images)\n",
    "\n",
    "**Copy the URL to your ChatBud UI!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_server"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHATBUD SERVER\n",
    "# - System prompt (child-safe)\n",
    "# - Conversation history (8K context)\n",
    "# - Image support\n",
    "# =============================================================================\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import threading\n",
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared 2>/dev/null\n",
    "!chmod +x cloudflared\n",
    "!fuser -k 5001/tcp 2>/dev/null || true\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "PORT = 5001\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "MAX_CONTEXT_TOKENS = 8192  # Gemma 3 4B max context\n",
    "MAX_NEW_TOKENS = 256       # Max response length\n",
    "MAX_HISTORY_TURNS = 20     # Keep last N conversation turns\n",
    "\n",
    "# Child-safe system prompt (HARDCODED - cannot be changed by users)\n",
    "SYSTEM_PROMPT = \"\"\"You are ChatBud, a friendly and safe helper for children aged 9\u201311.\n",
    "Speak with simple words (use the least number of words as possible) and short sentences (concise), like you're talking to a smart kid, and keep answers brief (about 1\u20134 short sentences as a maximum).\n",
    "Never swear, use rude or sexual language, or describe violence, self-harm, or sex in graphic detail.\n",
    "Do not give risky instructions, dares, or tips that could hurt someone in real life or online.\n",
    "If a problem sounds serious or scary, tell the child to stop, stay safe, and talk to a trusted adult such as a parent, caregiver, teacher, or counselor.\"\"\"\n",
    "\n",
    "# Store conversation history per session\n",
    "conversations = {}\n",
    "\n",
    "\n",
    "def decode_base64_image(base64_string):\n",
    "    \"\"\"Decode base64 image to PIL Image.\"\"\"\n",
    "    try:\n",
    "        if ',' in base64_string:\n",
    "            base64_string = base64_string.split(',')[1]\n",
    "        \n",
    "        image_bytes = base64.b64decode(base64_string)\n",
    "        image = Image.open(BytesIO(image_bytes)).convert(\"RGB\")\n",
    "        \n",
    "        # Resize large images\n",
    "        max_size = 512\n",
    "        if max(image.size) > max_size:\n",
    "            ratio = max_size / max(image.size)\n",
    "            new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n",
    "            image = image.resize(new_size, Image.LANCZOS)\n",
    "        \n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Image decode error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_messages(conversation_id, user_message, image=None):\n",
    "    \"\"\"\n",
    "    Build message list with system prompt and conversation history.\n",
    "    \"\"\"\n",
    "    # Get or create conversation history\n",
    "    if conversation_id not in conversations:\n",
    "        conversations[conversation_id] = []\n",
    "    \n",
    "    history = conversations[conversation_id]\n",
    "    \n",
    "    # Build messages list\n",
    "    messages = []\n",
    "    \n",
    "    # 1. System prompt (always first)\n",
    "    messages.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]\n",
    "    })\n",
    "    \n",
    "    # 2. Add conversation history (limit to last N turns)\n",
    "    recent_history = history[-MAX_HISTORY_TURNS:] if len(history) > MAX_HISTORY_TURNS else history\n",
    "    for turn in recent_history:\n",
    "        messages.append(turn)\n",
    "    \n",
    "    # 3. Add current user message\n",
    "    current_content = []\n",
    "    if image is not None:\n",
    "        current_content.append({\"type\": \"image\", \"image\": image})\n",
    "    \n",
    "    text = user_message if user_message else \"What do you see in this picture?\"\n",
    "    current_content.append({\"type\": \"text\", \"text\": text})\n",
    "    \n",
    "    current_user_msg = {\"role\": \"user\", \"content\": current_content}\n",
    "    messages.append(current_user_msg)\n",
    "    \n",
    "    return messages, current_user_msg\n",
    "\n",
    "\n",
    "def generate_response(conversation_id, user_message, image=None):\n",
    "    \"\"\"\n",
    "    Generate response with full conversation context.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Build messages with history\n",
    "        messages, current_user_msg = build_messages(conversation_id, user_message, image)\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = processor.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_dict=True,\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        input_length = inputs[\"input_ids\"].shape[-1]\n",
    "        print(f\"   Context: {input_length} tokens\")\n",
    "        \n",
    "        # Check if we're exceeding context limit\n",
    "        if input_length > MAX_CONTEXT_TOKENS - MAX_NEW_TOKENS:\n",
    "            print(f\"   \u26a0\ufe0f Context too long, trimming history...\")\n",
    "            # Remove oldest turns from history\n",
    "            if conversation_id in conversations and len(conversations[conversation_id]) > 2:\n",
    "                conversations[conversation_id] = conversations[conversation_id][-4:]\n",
    "                # Rebuild messages\n",
    "                messages, current_user_msg = build_messages(conversation_id, user_message, image)\n",
    "                inputs = processor.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=True,\n",
    "                    add_generation_prompt=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    return_dict=True,\n",
    "                ).to(\"cuda\")\n",
    "        \n",
    "        # Generate\n",
    "        with torch.inference_mode():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        response = processor.decode(\n",
    "            output_ids[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        # Save to history (without image data to save memory)\n",
    "        # Store user message\n",
    "        user_history_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": user_message or \"[sent an image]\"}]\n",
    "        }\n",
    "        conversations[conversation_id].append(user_history_msg)\n",
    "        \n",
    "        # Store assistant response\n",
    "        assistant_msg = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": response}]\n",
    "        }\n",
    "        conversations[conversation_id].append(assistant_msg)\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {e}\")\n",
    "        return \"Oops! Something went wrong. Can you try again?\"\n",
    "\n",
    "\n",
    "@app.route('/api/chat', methods=['POST'])\n",
    "def chat():\n",
    "    \"\"\"Handle chat messages.\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        message = data.get('message', '').strip()\n",
    "        image_data = data.get('image', None)\n",
    "        conversation_id = data.get('conversation_id', 'default')\n",
    "        \n",
    "        # Decode image if provided\n",
    "        image = None\n",
    "        if image_data:\n",
    "            print(\"\ud83d\udcf7 Image received\")\n",
    "            image = decode_base64_image(image_data)\n",
    "        \n",
    "        if not message and not image:\n",
    "            return jsonify({'success': False, 'error': 'No message or image'}), 400\n",
    "        \n",
    "        # Log\n",
    "        if image:\n",
    "            print(f\"\ud83d\udce9 [IMAGE] + '{message[:30]}...'\" if message else \"\ud83d\udce9 [IMAGE]\")\n",
    "        else:\n",
    "            print(f\"\ud83d\udce9 {message[:50]}\")\n",
    "        \n",
    "        # Generate with context\n",
    "        response = generate_response(conversation_id, message, image)\n",
    "        print(f\"\ud83d\udce4 {response[:50]}...\")\n",
    "        \n",
    "        return jsonify({'success': True, 'response': response})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error: {e}\")\n",
    "        return jsonify({'success': True, 'response': \"Something went wrong. Try again?\"})\n",
    "\n",
    "\n",
    "@app.route('/api/clear', methods=['POST'])\n",
    "def clear_history():\n",
    "    \"\"\"Clear conversation history.\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        conversation_id = data.get('conversation_id', 'default')\n",
    "        if conversation_id in conversations:\n",
    "            conversations[conversation_id] = []\n",
    "        print(f\"\ud83d\uddd1\ufe0f Cleared history for {conversation_id}\")\n",
    "        return jsonify({'success': True, 'message': 'History cleared'})\n",
    "    except Exception as e:\n",
    "        return jsonify({'success': False, 'error': str(e)})\n",
    "\n",
    "\n",
    "@app.route('/api/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({\n",
    "        'status': 'ok',\n",
    "        'model': 'Gemma 3 4B-IT + LoRA',\n",
    "        'features': ['text', 'images', 'memory'],\n",
    "        'max_context': MAX_CONTEXT_TOKENS,\n",
    "    })\n",
    "\n",
    "\n",
    "def run_flask():\n",
    "    app.run(host='0.0.0.0', port=PORT, use_reloader=False, threaded=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# START\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\ud83d\ude80 Starting ChatBud...\")\n",
    "print(f\"   Context window: {MAX_CONTEXT_TOKENS} tokens\")\n",
    "print(f\"   Max history: {MAX_HISTORY_TURNS} turns\\n\")\n",
    "\n",
    "flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
    "flask_thread.start()\n",
    "time.sleep(2)\n",
    "\n",
    "print(\"\ud83c\udf10 Starting tunnel...\\n\")\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    ['./cloudflared', 'tunnel', '--url', f'http://localhost:{PORT}'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "public_url = None\n",
    "for _ in range(60):\n",
    "    line = process.stdout.readline()\n",
    "    if line and 'trycloudflare.com' in line:\n",
    "        match = re.search(r'https://[a-zA-Z0-9-]+\\.trycloudflare\\.com', line)\n",
    "        if match:\n",
    "            public_url = match.group(0)\n",
    "            break\n",
    "\n",
    "if public_url:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"\ud83c\udf89 CHATBUD READY!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n\ud83d\udce1 URL: {public_url}\\n\")\n",
    "    print(\"\u2705 Features:\")\n",
    "    print(\"   \u2022 Child-safe system prompt (hardcoded)\")\n",
    "    print(\"   \u2022 Conversation memory (remembers context)\")\n",
    "    print(\"   \u2022 Image understanding\")\n",
    "    print(f\"   \u2022 {MAX_CONTEXT_TOKENS} token context window\")\n",
    "    print(\"\\n\ud83d\udccb Paste URL into ChatBud UI settings (\u2699\ufe0f)\")\n",
    "    print(\"\u26a0\ufe0f  Keep this cell running!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\ud83d\udc4b Bye!\")\n",
    "else:\n",
    "    print(\"\u274c Tunnel failed. Run cell again.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}