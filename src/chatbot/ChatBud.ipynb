{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# ChatBud - Child-Friendly AI Assistant\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell1_header"
      },
      "source": [
        "## Cell 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers>=4.50.0 accelerate>=1.0.0 peft>=0.12.0 bitsandbytes>=0.43.3\n",
        "!pip install -q huggingface_hub sentencepiece\n",
        "!pip install -q flask flask-cors Pillow\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    raise RuntimeError(\"‚ùå No GPU! Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell2_header"
      },
      "source": [
        "## Cell 2: Login to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf_login"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "except:\n",
        "    hf_token = None\n",
        "\n",
        "if not hf_token:\n",
        "    hf_token = \"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\"\n",
        "\n",
        "login(token=hf_token)\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n",
        "print(\"‚úÖ Logged in to Hugging Face\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell3_header"
      },
      "source": [
        "## Cell 3: Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "ADAPTER_DIR = \"/content/drive/MyDrive/gemma3_child_friendly_lora/gemma3_child_friendly_lora\"\n",
        "\n",
        "if os.path.exists(ADAPTER_DIR):\n",
        "    print(f\"‚úÖ Adapter found: {ADAPTER_DIR}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"‚ùå Adapter not found: {ADAPTER_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell4_header"
      },
      "source": [
        "## Cell 4: Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_base_model"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, Gemma3ForConditionalGeneration, BitsAndBytesConfig\n",
        "\n",
        "MODEL_ID = \"google/gemma-3-4b-it\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"Loading processor...\")\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "print(\"‚úÖ Processor loaded\")\n",
        "\n",
        "print(\"\\nLoading model (2-3 min)...\")\n",
        "base_model = Gemma3ForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "base_model.eval()\n",
        "print(\"‚úÖ Base model loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell5_header"
      },
      "source": [
        "## Cell 5: Load LoRA + Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_lora"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "print(f\"Loading LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
        "model.eval()\n",
        "print(\"‚úÖ Fine-tuned model ready!\")\n",
        "\n",
        "# =============================================================================\n",
        "# SYSTEM PROMPT (hardcoded for child safety)\n",
        "# =============================================================================\n",
        "SYSTEM_PROMPT = \"\"\"You are ChatBud, a friendly and safe helper for children aged 9‚Äì11.\n",
        "Speak with simple words (use the least number of words as possible) and short sentences (concise), like you're talking to a smart kid, and keep answers brief (about 1‚Äì4 short sentences as a maximum).\n",
        "Never swear, use rude or sexual language, or describe violence, self-harm, or sex in graphic detail.\n",
        "Do not give risky instructions, dares, or tips that could hurt someone in real life or online.\n",
        "If a problem sounds serious or scary, tell the child to stop, stay safe, and talk to a trusted adult such as a parent, caregiver, teacher, or counselor.\"\"\"\n",
        "\n",
        "print(f\"\\nüìã System prompt loaded ({len(SYSTEM_PROMPT)} chars)\")\n",
        "\n",
        "# =============================================================================\n",
        "# Test with system prompt\n",
        "# =============================================================================\n",
        "print(\"\\nüß™ Testing model with system prompt...\")\n",
        "try:\n",
        "    test_messages = [\n",
        "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]},\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What are elephants?\"}]}\n",
        "    ]\n",
        "    \n",
        "    test_inputs = processor.apply_chat_template(\n",
        "        test_messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True,\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    print(f\"   Input tokens: {test_inputs['input_ids'].shape[-1]}\")\n",
        "    \n",
        "    with torch.inference_mode():\n",
        "        test_output = model.generate(\n",
        "            **test_inputs,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "    \n",
        "    response = processor.decode(test_output[0][test_inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
        "    print(f\"   ‚úÖ Response: {response[:100]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Test failed: {e}\")\n",
        "    raise\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Ready! Run Cell 6 to start server.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell6_header"
      },
      "source": [
        "## Cell 6: Start Server (with Memory + Images)\n",
        "\n",
        "**Copy the URL to your ChatBud UI!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_server"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CHATBUD SERVER\n",
        "# - System prompt (child-safe)\n",
        "# - Conversation history (8K context)\n",
        "# - Image support\n",
        "# =============================================================================\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import base64\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "import re\n",
        "\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared 2>/dev/null\n",
        "!chmod +x cloudflared\n",
        "!fuser -k 5001/tcp 2>/dev/null || true\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "PORT = 5001\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "MAX_CONTEXT_TOKENS = 8192  # Gemma 3 4B max context\n",
        "MAX_NEW_TOKENS = 256       # Max response length\n",
        "MAX_HISTORY_TURNS = 20     # Keep last N conversation turns\n",
        "\n",
        "# Child-safe system prompt (HARDCODED - cannot be changed by users)\n",
        "SYSTEM_PROMPT = \"\"\"You are ChatBud, a friendly and safe helper for children aged 9‚Äì11.\n",
        "Speak with simple words (use the least number of words as possible) and short sentences (concise), like you're talking to a smart kid, and keep answers brief (about 1‚Äì4 short sentences as a maximum).\n",
        "Never swear, use rude or sexual language, or describe violence, self-harm, or sex in graphic detail.\n",
        "Do not give risky instructions, dares, or tips that could hurt someone in real life or online.\n",
        "If a problem sounds serious or scary, tell the child to stop, stay safe, and talk to a trusted adult such as a parent, caregiver, teacher, or counselor.\"\"\"\n",
        "\n",
        "# Store conversation history per session\n",
        "conversations = {}\n",
        "\n",
        "\n",
        "def decode_base64_image(base64_string):\n",
        "    \"\"\"Decode base64 image to PIL Image.\"\"\"\n",
        "    try:\n",
        "        if ',' in base64_string:\n",
        "            base64_string = base64_string.split(',')[1]\n",
        "        \n",
        "        image_bytes = base64.b64decode(base64_string)\n",
        "        image = Image.open(BytesIO(image_bytes)).convert(\"RGB\")\n",
        "        \n",
        "        # Resize large images\n",
        "        max_size = 512\n",
        "        if max(image.size) > max_size:\n",
        "            ratio = max_size / max(image.size)\n",
        "            new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n",
        "            image = image.resize(new_size, Image.LANCZOS)\n",
        "        \n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Image decode error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def build_messages(conversation_id, user_message, image=None):\n",
        "    \"\"\"\n",
        "    Build message list with system prompt and conversation history.\n",
        "    \"\"\"\n",
        "    # Get or create conversation history\n",
        "    if conversation_id not in conversations:\n",
        "        conversations[conversation_id] = []\n",
        "    \n",
        "    history = conversations[conversation_id]\n",
        "    \n",
        "    # Build messages list\n",
        "    messages = []\n",
        "    \n",
        "    # 1. System prompt (always first)\n",
        "    messages.append({\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]\n",
        "    })\n",
        "    \n",
        "    # 2. Add conversation history (limit to last N turns)\n",
        "    recent_history = history[-MAX_HISTORY_TURNS:] if len(history) > MAX_HISTORY_TURNS else history\n",
        "    for turn in recent_history:\n",
        "        messages.append(turn)\n",
        "    \n",
        "    # 3. Add current user message\n",
        "    current_content = []\n",
        "    if image is not None:\n",
        "        current_content.append({\"type\": \"image\", \"image\": image})\n",
        "    \n",
        "    text = user_message if user_message else \"What do you see in this picture?\"\n",
        "    current_content.append({\"type\": \"text\", \"text\": text})\n",
        "    \n",
        "    current_user_msg = {\"role\": \"user\", \"content\": current_content}\n",
        "    messages.append(current_user_msg)\n",
        "    \n",
        "    return messages, current_user_msg\n",
        "\n",
        "\n",
        "def generate_response(conversation_id, user_message, image=None):\n",
        "    \"\"\"\n",
        "    Generate response with full conversation context.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Build messages with history\n",
        "        messages, current_user_msg = build_messages(conversation_id, user_message, image)\n",
        "        \n",
        "        # Process inputs\n",
        "        inputs = processor.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "            return_dict=True,\n",
        "        ).to(\"cuda\")\n",
        "        \n",
        "        input_length = inputs[\"input_ids\"].shape[-1]\n",
        "        print(f\"   Context: {input_length} tokens\")\n",
        "        \n",
        "        # Check if we're exceeding context limit\n",
        "        if input_length > MAX_CONTEXT_TOKENS - MAX_NEW_TOKENS:\n",
        "            print(f\"   ‚ö†Ô∏è Context too long, trimming history...\")\n",
        "            # Remove oldest turns from history\n",
        "            if conversation_id in conversations and len(conversations[conversation_id]) > 2:\n",
        "                conversations[conversation_id] = conversations[conversation_id][-4:]\n",
        "                # Rebuild messages\n",
        "                messages, current_user_msg = build_messages(conversation_id, user_message, image)\n",
        "                inputs = processor.apply_chat_template(\n",
        "                    messages,\n",
        "                    tokenize=True,\n",
        "                    add_generation_prompt=True,\n",
        "                    return_tensors=\"pt\",\n",
        "                    return_dict=True,\n",
        "                ).to(\"cuda\")\n",
        "        \n",
        "        # Generate\n",
        "        with torch.inference_mode():\n",
        "            output_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "            )\n",
        "        \n",
        "        # Decode response\n",
        "        response = processor.decode(\n",
        "            output_ids[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "            skip_special_tokens=True\n",
        "        ).strip()\n",
        "        \n",
        "        # Save to history (without image data to save memory)\n",
        "        # Store user message\n",
        "        user_history_msg = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": user_message or \"[sent an image]\"}]\n",
        "        }\n",
        "        conversations[conversation_id].append(user_history_msg)\n",
        "        \n",
        "        # Store assistant response\n",
        "        assistant_msg = {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": response}]\n",
        "        }\n",
        "        conversations[conversation_id].append(assistant_msg)\n",
        "        \n",
        "        return response\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Generation error: {e}\")\n",
        "        return \"Oops! Something went wrong. Can you try again?\"\n",
        "\n",
        "\n",
        "@app.route('/api/chat', methods=['POST'])\n",
        "def chat():\n",
        "    \"\"\"Handle chat messages.\"\"\"\n",
        "    try:\n",
        "        data = request.json\n",
        "        message = data.get('message', '').strip()\n",
        "        image_data = data.get('image', None)\n",
        "        conversation_id = data.get('conversation_id', 'default')\n",
        "        \n",
        "        # Decode image if provided\n",
        "        image = None\n",
        "        if image_data:\n",
        "            print(\"üì∑ Image received\")\n",
        "            image = decode_base64_image(image_data)\n",
        "        \n",
        "        if not message and not image:\n",
        "            return jsonify({'success': False, 'error': 'No message or image'}), 400\n",
        "        \n",
        "        # Log\n",
        "        if image:\n",
        "            print(f\"üì© [IMAGE] + '{message[:30]}...'\" if message else \"üì© [IMAGE]\")\n",
        "        else:\n",
        "            print(f\"üì© {message[:50]}\")\n",
        "        \n",
        "        # Generate with context\n",
        "        response = generate_response(conversation_id, message, image)\n",
        "        print(f\"üì§ {response[:50]}...\")\n",
        "        \n",
        "        return jsonify({'success': True, 'response': response})\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return jsonify({'success': True, 'response': \"Something went wrong. Try again?\"})\n",
        "\n",
        "\n",
        "@app.route('/api/clear', methods=['POST'])\n",
        "def clear_history():\n",
        "    \"\"\"Clear conversation history.\"\"\"\n",
        "    try:\n",
        "        data = request.json\n",
        "        conversation_id = data.get('conversation_id', 'default')\n",
        "        if conversation_id in conversations:\n",
        "            conversations[conversation_id] = []\n",
        "        print(f\"üóëÔ∏è Cleared history for {conversation_id}\")\n",
        "        return jsonify({'success': True, 'message': 'History cleared'})\n",
        "    except Exception as e:\n",
        "        return jsonify({'success': False, 'error': str(e)})\n",
        "\n",
        "\n",
        "@app.route('/api/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\n",
        "        'status': 'ok',\n",
        "        'model': 'Gemma 3 4B-IT + LoRA',\n",
        "        'features': ['text', 'images', 'memory'],\n",
        "        'max_context': MAX_CONTEXT_TOKENS,\n",
        "    })\n",
        "\n",
        "\n",
        "def run_flask():\n",
        "    app.run(host='0.0.0.0', port=PORT, use_reloader=False, threaded=True)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# START\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üöÄ Starting ChatBud...\")\n",
        "print(f\"   Context window: {MAX_CONTEXT_TOKENS} tokens\")\n",
        "print(f\"   Max history: {MAX_HISTORY_TURNS} turns\\n\")\n",
        "\n",
        "flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
        "flask_thread.start()\n",
        "time.sleep(2)\n",
        "\n",
        "print(\"üåê Starting tunnel...\\n\")\n",
        "\n",
        "process = subprocess.Popen(\n",
        "    ['./cloudflared', 'tunnel', '--url', f'http://localhost:{PORT}'],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "public_url = None\n",
        "for _ in range(60):\n",
        "    line = process.stdout.readline()\n",
        "    if line and 'trycloudflare.com' in line:\n",
        "        match = re.search(r'https://[a-zA-Z0-9-]+\\.trycloudflare\\.com', line)\n",
        "        if match:\n",
        "            public_url = match.group(0)\n",
        "            break\n",
        "\n",
        "if public_url:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üéâ CHATBUD READY!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nüì° URL: {public_url}\\n\")\n",
        "    print(\"‚úÖ Features:\")\n",
        "    print(\"   ‚Ä¢ Child-safe system prompt (hardcoded)\")\n",
        "    print(\"   ‚Ä¢ Conversation memory (remembers context)\")\n",
        "    print(\"   ‚Ä¢ Image understanding\")\n",
        "    print(f\"   ‚Ä¢ {MAX_CONTEXT_TOKENS} token context window\")\n",
        "    print(\"\\nüìã Paste URL into ChatBud UI settings (‚öôÔ∏è)\")\n",
        "    print(\"‚ö†Ô∏è  Keep this cell running!\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    try:\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nüëã Bye!\")\n",
        "else:\n",
        "    print(\"‚ùå Tunnel failed. Run cell again.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
